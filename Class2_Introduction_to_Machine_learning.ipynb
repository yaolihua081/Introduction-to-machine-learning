{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Class2_Introduction_to_Machine_learning.ipynb","provenance":[],"collapsed_sections":["q7EBurthhcQn","I0bQEq4si_Ly","UgsDYktAfJPk","M2DSR44pmLtL"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Text Book: \n","#An Introduction to Statistical Learning \n","Gareth James â€¢ Daniela Witten â€¢ Trevor Hastie Robert Tibshirani"],"metadata":{"id":"aSlQelyXHjkS"}},{"cell_type":"markdown","source":["# Review and Questions from Class1\n"],"metadata":{"id":"soO8fexAeoM0"}},{"cell_type":"markdown","source":["#**Two Purpose of the class:**\n","1. What is machine learning?\n","2. How to solve real life problems with machine learning?"],"metadata":{"id":"gMb7ZH-XZPBg"}},{"cell_type":"markdown","source":["## 1. **Introduction to Machine learning** \n","\n","\n","*   Machine learning Applications:\n","\n","1.   Submitting a query to a search engine,\n","2.   Imagine recognation.\n","3.   Recomender system.\n","4.   Automatic translation.\n","5.   Automatic scoring.\n","6.   Stock prediction.\n","*  Input---->Machine Learning----->output"],"metadata":{"id":"tmfsuGguMASW"}},{"cell_type":"markdown","source":["### 1.1 **Input/Data**\n","\n","Test responses, house price, stock prices each day, imagines, essays, a passage..."],"metadata":{"id":"RNeAWJA6VpZ0"}},{"cell_type":"markdown","source":["### 1.2 **Machine Learning Algorithms**\n","\n","Define **a loss function** and find a way to minimize it with evaluation criteria.\n","\n","Some examples of functions to be maximining or minimizing and algorithms:\n","\n","1.Maximum likelihood estimation (MLE) is a method that determines values for the parameters of a model. The parameter values are found such that they maximise the likelihood that the process described by the model produced the data that were actually observed. See below.\n","\n","2.Least squares estimaiton:\n","Least squares minimisation is another common method for estimating parameter values for a model in machine learning. It turns out that when the model is assumed to be Gaussian, the MLE estimates are equivalent to the least squares method.\n","\n","3.Expectation-Maximization algorithms, an iterative solution: derivative for the likelihood function is hard to find, using EM is the way to go to maximizing the function:\n","\n","E-Step. Estimate the missing variables in the dataset.\n","M-Step. Maximize the parameters of the model in the presence of the data.\n","\n","\n","4.Bayes theorem:\n","\n","$P(A \\mid  B) = P(B \\mid A) * P(A)$.\n","\n","We want to find $\\theta$, so as to maximizing $P(X \\mid \\theta)$. It is the same as maximizing $P(\\theta \\mid  X)$, because \n","$P(\\theta \\mid  X) = P(X \\mid \\theta) * P(\\theta)$. Here $P(\\theta)$ is a prior distribution for the parameter $\\theta$. \n","\n","5.MCMC: Markov chain Monte Carlo (MCMC) methods comprise a class of algorithms for sampling from a probability distribution/ likelihood function: aiming for the sample matches the actual desired distribution.\n","\n","\n"],"metadata":{"id":"dR-6BwWqVfqh"}},{"cell_type":"markdown","source":["###1.3 **Supervised vs. Unsupervised**\n","\n","Supervised statistical learning: to build a statistical model \n","for predicting, or estimating, with an output based on one or more inputs. Problems of this nature occur in fields as diverse as business, medicine, astrophysics, and public policy.  With independent, observable predictor variables $X$, there are responses Y, we need to fit a model that relates the response Y to the predictors X, with the aim of accurately predicting the response for future observations (called prediction) or better understanding the relationship between the response and the predictors (called inference).\n","\n","\n","Unsupervised statistical learning: there are inputs $X$ but no supervising output $Y$. We only have observale varibales $X$, and with the data, we are to learn relationships and structures between the variables $X$. Example: cluster users/customers.\n","\n","For example, in a market segmentation study we might observe multiple characteristics (variables) for potential customers, such as zip code, family income, and shopping habits. We might believe that the customers fall into different groups, such as big spenders versus low spenders. If the information about each customerâ€™s spending patterns were available, then a supervised analysis would be possible. However, this information is not availableâ€”that is, we do not know whether each potential customer is a big spender or not. In this setting, we can try to cluster the customers on the basis of the variables measured, in order to identify distinct groups of potential customers. Identifying such groups can be of interest because it might be that the groups differ with respect to some property of interest, such as spending habits.\n","\n","Semi-supervised learning problem: there are some m observations with response Y, but there are n-m observations that have no response value; mnay times the collection of Y is expensive or difficult.\n","\n","We will have applications for both supervised and unsupervised."],"metadata":{"id":"lKpr177m-WQa"}},{"cell_type":"markdown","source":["### 1.4 Output data\n","**bold text**\n"," Classification(image recognitation, disease, pass/fail, YES/No), Prediction(stock price, students attribute estimates, sale price)"],"metadata":{"id":"LElg7dRWBq4S"}},{"cell_type":"markdown","source":["## 2.**Statistics**"],"metadata":{"id":"3keW8omWU3zf"}},{"cell_type":"markdown","source":["### 2.1 **Dependent Variable and Independent Variables.**\n","\n","Let Y, a dependent variable, be an observed quantitative variable. $X = (X_{1},X_{2},...,X_{p})$ where $X_{1}, X_{2}, . . . , X_{p}$ are p independent variables. We assume that there is some relationship between Y and $X = (X_{1},X_{2},...,X_{p})$, which can be written in the very general form\n","$Y =f(X)+Ïµ$. Here f is some fixed but unknown function of $X_{1}, X_{2}, . . . , X_{p}$, and $Ïµ$ is a random error term, which is independent of X and has mean zero. In this formulation, f represents the systematic information that X provides about Y.\n","\n","We may wish to estimate f: prediction and inference. \n","\n","For example, X=(maybe years of education, age), Y is his/her salary. Using observed data, we can use statistical tool to predict the salary Y for some people given their education and age. We may also wants to know how the years of education affect one's salary.\n","\n","What is exactly $f(x)$? \n","\n","In many situations, we do not care (it is like a black box); we just need to have a good prediction/estimates. In other situations, we may want to know exactly what is $f(x)$, so that we know how those varibales $X = (X_{1},X_{2},...,X_{p})$ change affect our resppnses/outcome or the values of Y. What is the relationship between the response and each predictor?\n","\n","\n","\n","How to find a good prediction for Y:\n","1. Aiming for the estimation of function $f(x)$. Try to find better model and methods, to reduce the estimation error.\n","2. However, there is some irreducible errors from $Ïµ$. \n","\n","Therefore, our prediction would always have some error in it.\n","\n","\n","\n"],"metadata":{"id":"39gC_Bw2fjIr"}},{"cell_type":"markdown","source":["###2.2 **Statistical Models and Methods**\n","\n","How to estimates $f(x)$\n","1. **Parametric Methods.** For example, we may assume\n","$f(X)=\\beta_{0}+\\beta_{1}*X_{1}+\\beta_{2}*X_{2}+....\\beta_{p}*X_{p}$. We need to estimate the unknown parameters $\\beta_{0},....\\beta_{p}$.\n","\n","2. **Non-parametric Methods.** Here, we do not make explicit assumptions about the functional form of $f(x)$. We search for an estimate of $f(x)$ that gets as close to the data points as possible. We also want the estimates to be as smooth as possible.\n","\n","Parametric Methods are quite easy to understand the relationship between Y and $X = (X_{1},X_{2},...,X_{p})$.\n","Non-parametric Methods are like black box, not easy to interprete.\n","\n","We try to seek models and methods with the trade-off between flexibility and interpretability.\n","\n","A simple, easy to understand and interpretable, less flexible model may have bigger errors, while a flexible, less to understand model may have smaller errors.\n","\n","Two concepts we need to know in statistics: Mean and Variance of a random variable.\n","\n","We define the mean of a random variable X as\n","$EX=âˆ«xdp(x)$, and for a function $f$,\n","$E[f(X)]=âˆ« f(x)dp(x)$. X is a discrete random variable, then the mean or expection is $EX=âˆ‘_{x} xp(x)$.\n","\n","The variance of a random variable\n","X is defined as $Var[X]=E(X-EX)^{2}$."],"metadata":{"id":"C3w1y_B8gJkb"}},{"cell_type":"markdown","source":["###2.3 **Bias and Variances**\n","\n","How to measure the quality of the prediction?\n","\n","In the regression setting, the\n","most commonly-used measure is the mean squared error (MSE), given by mean\n","\\begin{align}\n","MSE = 1/n âˆ‘_{i=1}^{n}(y_{i} âˆ’\\hat{f}(x_{i}))^{2} ,\n","\\end{align}\n","\n","Many statistical methods specifically estimate coefficients so as to minimize the training set MSE. For these methods, the training set MSE can be quite small, but the test MSE is often much larger.\n","\n","Training set: data that has both $X$ and $Y$, and are used to derive predictions or inferences. \n","\n","Test set: the predictions are derived based on the observable data $X$ uisng the model derived from the traing set data.\n","A good model is one that yield small test MSE.\n","\n","In practice, cross-validation is used to select a method that minimizes the MSE for the validation data; hope the derived model will yield small test MSE.\n","\n","For classification setting, the error rate is defined as\n","\\begin{align}\n","MSE = 1/n âˆ‘_{i=1}^{n}I(y_{i} != \\hat{y}_{i})\n","\\end{align}\n","where $I(y_{i} != \\hat{y}_{i})$ is an indicator variable that equals 1 if $y_{i} != \\hat{y}_{i}$   and zero otherwise.\n","\n","A good classifier is one for which the test error above is smallest.\n","\n","The expected test MSE has three parts: Bias ($\\hat{f}(x_{i})$, Variance, and irreducible error term $var(Ïµ)$. \n","\\begin{align}\n","MSE(\\hat{\\theta})=E(\\hat{\\theta}-\\theta)^{2}=E(\\hat{\\theta}-E(\\hat{\\theta})+E(\\hat{\\theta})-\\theta)^{2}\n","\\end{align}\n","\\begin{align}\n","=E(\\hat{\\theta}-E(\\hat{\\theta}))^{2}+E[E(\\hat{\\theta})-\\theta]^{2}\n","\\end{align}\n","\\begin{align}\n","=VAR(\\hat{\\theta})+[E(\\hat{\\theta})-\\theta]^{2}=Variance+SQUAREDBIAS\n","\\end{align}\n","\n","\n","**The Bias-Variance Trade-Off**\n","\n","Variance refers to the amount by which $\\hat{f}$ would change if we estimated it using a different training data set. In general, more flexible statistical methods have higher variance. \n","\n","Bias refers to the error that is introduced by approximating a real life problem, which may be extremely complicated, by a much simpler model. For example, linear regression assumes that there is a linear relationship between Y and $X_{1}, X_{2}, . . . , X_{p}$. It is unlikely that any real life problem truly has such a simple linear relationship, and so performing linear regression will undoubtedly result in some bias in the estimate of f. \n","\n","As a general rule, as we use more flexible methods, the variance will increase and the bias will decrease.\n","\n","In general, when inference is the goal, there are clear advantages to using simple and relatively inflexible statistical learning methods. However, we will often obtain more accurate predictions using a less flexible method too!\n","        \n","    "],"metadata":{"id":"gC3QOiUtvpaz"}},{"cell_type":"markdown","source":["###2.5 **Regression Versus Classification Problems**\n","\n","Variables can be characterized as either quantitative or qualitative (also known as categorical). \n","\n","Quantitative variables take on numerical values. Examples include a personâ€™s age, height, or income, the value of a house, and the price of a stock. \n","\n","In contrast, qualitative variables take on values in one of K different classes, or categories. Examples of qualitative variables include a personâ€™s gender (male or female), the brand of product purchased (brand A, B, or C), stock increase or decrease for centain day.\n","a person defaults on a debt (yes or no),\n","cancer diagnosis (Acute Myelogenous Leukemia, Acute Lymphoblastic Leukemia, or No Leukemia). \n","\n","\n","We tend to refer to problems with a quantitative response as regression problems, while those involving a qualitative response are often referred to as classification problems. \n","\n","\n","However, the distinction is not always that crisp. Least squares linear regression is used with a quantitative response, whereas logistic regression is typically used with a qualitative (two-class, or binary) response. As such it is often used as a classification method. But since it estimates class probabilities, it can be thought of as a regression.\n","\n","We will have applications for both regression and classification."],"metadata":{"id":"OStWJe9JmFaK"}},{"cell_type":"markdown","source":["## **First simple application of model fit and estimation, and likelihood function.**\n","\n","Gaussian distribution has 2 parameters. The mean, Î¼, and the standard deviation, Ïƒ. Maximum likelihood estimation is a method that will find the values of Î¼ and Ïƒ that result in the curve that best fits the data.\n","For observations $X_{1}, \\cdots, X_{n}$ from Gaussian with mean, Î¼, and the standard deviation, Ïƒ, the probability of the event happened is\n","\n","$L(X, \\mu, \\sigma)=P(X_{1},\\cdots,X_{n} \\mid \\mu, \\sigma)=âˆ_{i=1}^{n}P(X_{i} \\mid \\mu, \\sigma)$.\n","\n","To solove for the maximum, take log and take derivative swith respect to $\\mu$ and $\\sigma$, we will have the estimates\n","$\\hat{\\mu}=\\overline{X}=\\frac{1}{n}âˆ‘_{i=1}^{n}X_{i}$,\n","$\\hat{\\sigma}^{2}=Var(X)=\\sum_{i=1}^{n} \\frac{(X_{i}-\\overline{X})^{2}}{n}$\n","\n","$Var(\\hat{\\mu})=SE(\\hat{\\mu})=Var(\\frac{1}{n}âˆ‘_{i=1}^{n}X_{i})=\\frac{\\sigma^{2}}{n}$\n","\n","Normal density function is\n","$p(x)=\\frac{1}{\\sigma \\sqrt{2\\pi}}exp^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^{2}}$"],"metadata":{"id":"q7EBurthhcQn"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import seaborn as sns; sns.set()\n","import numpy as np\n","# For dataframes\n","import pandas as pd \n"],"metadata":{"id":"G0yMuDna4e9v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Given a function, with known parameters, we can plot them: random draw samples from the distribution and plot the samples.\n","### How do we find the unknown parameters for a function, given the obervations of sample data from the function?"],"metadata":{"id":"I0bQEq4si_Ly"}},{"cell_type":"code","source":["x"],"metadata":{"id":"eMWaG9M5cPMo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","mu, sigma = 0, 10 # mean and standard deviation\n","n=1000\n","x = np.random.normal(mu, sigma, n)\n","\n","count, bins, ignored = plt.hist(x, 30, density=True)\n","plt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) *\n","               np.exp( - (bins - mu)**2 / (2 * sigma**2) ),\n","         linewidth=2, color='r')\n","\n","plt.ylabel('density')\n","plt.xlabel('x')\n","plt.title(\"Normal Distribution\")\n","plt.show()"],"metadata":{"id":"5CeyjcQKtlbm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x.mean(),np.mean(x), np.std(x),x.max(), x.argmax()# returns indices of the max element of the array,\n","\n"],"metadata":{"id":"7NqPkmdghFWg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1644023863038,"user_tz":480,"elapsed":247,"user":{"displayName":"lihua yao","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg5rCuL_iDZvHl-VqYRc6KCdT0FRCd54wGgy-Ejpg=s64","userId":"00281987792034256062"}},"outputId":"5def7581-1467-4a81-83df-4828aab032c5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(0.00403202863844546,\n"," 0.00403202863844546,\n"," 1.0368005811497654,\n"," 3.1922110414267384,\n"," 55)"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":[""],"metadata":{"id":"g0AUNeMBiAAM"}},{"cell_type":"code","source":["X = np.random.uniform(-2,2, n)\n","X"],"metadata":{"id":"dPaDEVrReorP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X = np.random.uniform(-2,2, n)\n","Z=2+3*X"],"metadata":{"id":"b8ZMs50weoLx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mu, sigma = 0, 2 # mean and standard deviation\n","n=100\n","ðœ– = np.random.normal(mu, sigma, n)\n","X = np.random.uniform(-2,2, n)\n","Y=2+3*X+ðœ–\n","Z=2+3*X\n","Y[0]=9\n","df = pd.DataFrame({'x':X, 'y':Y,'z':Z})\n","plt.plot('x', 'z',data=df,color='red')\n","plt.scatter('x','y',data=df,color='blue')\n","plt.title(\"Linear Regression\")\n","plt.show()"],"metadata":{"id":"q0Lb_1shiHJi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##**Overfitting and underfitting**"],"metadata":{"id":"mw3xBsoG5r08"}},{"cell_type":"markdown","source":["##np.random.randn(d0,d1),  Return a sample (or samples) from the â€œstandard normalâ€ distribution.\n","##np.random.rand(d0,d1),  Return a sample (or samples) of random values in (0,1) "],"metadata":{"id":"t7U_vV1F9dtL"}},{"cell_type":"code","source":["\n","\n","def make_data(N=30, err=0.8, rseed=1):\n","    # randomly sample the data\n","    rng = np.random.RandomState(rseed)\n","    X = rng.rand(N, 1) ** 2\n","    y = 10 - 1. / (X.ravel() + 0.1)\n","    if err > 0:\n","        y += err * rng.randn(N)\n","    return X, y"],"metadata":{"id":"KjENmkTX4wOk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.preprocessing import PolynomialFeatures\n","from sklearn.linear_model import LinearRegression\n","from sklearn.pipeline import make_pipeline\n","\n","def PolynomialRegression(degree=2, **kwargs):\n","    return make_pipeline(PolynomialFeatures(degree),\n","                         LinearRegression(**kwargs))"],"metadata":{"id":"5qzVWDji5N0f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X, y = make_data()\n","#xfit = np.linspace(-0.1, 1.0, 1000)[:, None]\n","xfit=np.linspace(-0.1, 1.0, 1000).reshape(-1, 1)\n","model1 = PolynomialRegression(1).fit(X, y)\n","model20 = PolynomialRegression(20).fit(X, y)\n","\n","fig, ax = plt.subplots(1, 2, figsize=(10, 6))\n","#fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n","\n","ax[0].scatter(X.ravel(), y, s=40)\n","ax[0].plot(xfit.ravel(), model1.predict(xfit), color='gray')\n","ax[0].axis([-0.1, 1.0, -2, 14])\n","ax[0].set_title('High-bias model: Underfits the data', size=14)\n","\n","ax[1].scatter(X.ravel(), y, s=40)\n","ax[1].plot(xfit.ravel(), model20.predict(xfit), color='gray')\n","ax[1].axis([-0.1, 1.0, -2, 14])\n","ax[1].set_title('High-variance model: Overfits the data', size=14)\n","plt.show()"],"metadata":{"id":"MssQmmcs5ZJK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. Computer programing"],"metadata":{"id":"UgsDYktAfJPk"}},{"cell_type":"markdown","source":["3.1 Python, R, Java, C, C++\n","\n","3.2 Define a function that need to be optimized: minimum or maximinum\n","\n","3.3 Algorithem to solve the optimized problem.\n","\n","3.4 Measurement error"],"metadata":{"id":"c1XiS0cBfTUs"}},{"cell_type":"markdown","source":["##**HW Project, Practice python**"],"metadata":{"id":"M2DSR44pmLtL"}},{"cell_type":"markdown","source":["###HW, read Introduction to Styatistical learning, chapter 1\n","\n","1. For each of parts (a) through (d), indicate whether we would generally expect the performance of a flexible statistical learning method to be better or worse than an inflexible method. Justify your answer.\n","\n","(a) The sample size n is extremely large, and the number of predictors p is small.\n","\n","(b) The number of predictors p is extremely large, and the number of observations n is small.\n","\n","(c) The relationship between the predictors and response is highly non-linear.\n","\n","(d) The variance of the error terms, i.e. Ïƒ2 = Var(Îµ), is extremely high.\n","\n","\n","2. Explain whether each scenario is a classification or regression problem, and indicate whether we are most interested in inference or prediction. Finally, provide n and p.\n","\n","(a) We collect a set of data on the top 500 firms in the US. For each firm we record profit, number of employees, industry and the CEO salary. We are interested in understanding which factors affect CEO salary.\n","\n","(b) We are considering launching a new product and wish to know whether it will be a success or a failure. We collect data on 20 similar products that were previously launched. For each product we have recorded whether it was a success or failure, price charged for the product, marketing budget, competition price, and ten other variables.\n","\n","(c) We are interesting in predicting the % change in the US dollar in relation to the weekly changes in the world stock markets. Hence we collect weekly data for all of 2012. For each week we record the % change in the dollar, the % change in the US market, the % change in the British market, and the % change in the German market.\n","\n","3. Describe the differences between a parametric and a non-parametric statistical learning approach. What are the advantages of a parametric approach to regression or classification (as opposed to a nonparametric approach) ? What are its disadvantages ?\n","\n","4. What are the advantages and disadvantages of a very flexible (versus a less flexible) approach for regression or classification ? Under what circumstances might a more flexible approach be preferred to a less flexible approach? When might a less flexible approach be preferred ?\n"],"metadata":{"id":"WA8OZou2LZfS"}},{"cell_type":"markdown","source":["### Answer\n","\n","1.\n","\n","a)A flexible method will fit the data closer and with the large sample size, would perform better than an inflexible approach.\n","\n","b)A flexible method would overfit the small number of observations, so worse then inflexible method.\n","\n","c) With more degrees of freedom, a flexible method would fit better than an inflexible one.\n","\n","d) A flexible method would fit to the noise in the error terms and increase variance.\n","\n","\n","2.\n","a)Regression and inference with n=500 and p=3\n","\n","b) Classification and prediction with n=20 and p=13\n","\n","c) Regression and prediction with n=52 and p=3\n","\n","3.\n","A parametric approach reduces the problem of estimating f down to one of estimating a set of parameters because it assumes a form for f.\n","\n","A non-parametric approach does not assume a patricular form of f and so requires a very large sample to accurately estimate f.\n","\n","The advantages of a parametric approach to regression or classification are the simplifying of modeling f to a few parameters and not as many observations are required compared to a non-parametric approach.\n","\n","The disadvantages of a parametric approach to regression or classification are a potentially inaccurate estimate f if the form of f assumed is wrong or to overfit the observations if more flexible models are used.\n","\n","4.\n","\n","The advantages of a very flexible approach are that it may give a better fit for non-linear models and it decreases the bias.\n","\n","The disadvantages of a very flexible approach are that it requires estimating a greater number of parameters, it follows the noise too closely (overfit) and it increases the variance.\n","\n","A more flexible approach would be preferred to a less flexible approach when we are interested in prediction and not the interpretability of the results.\n","\n","A less flexible approach would be preferred to a more flexible approach when we are interested in inference and the interpretability of the results.\n"],"metadata":{"id":"SAnB0Kd-GJSV"}}]}