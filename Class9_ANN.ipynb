{"cells":[{"cell_type":"markdown","metadata":{"id":"eaPWLUBq4P0x"},"source":["# 03/25/22 Artificial Neural Network (ANN) in Machine Learning\n","\n","ANN is for machine learning as well as pattern recognition, based on the model of a human neuron. The human brain consists of millions of neurons. \n","A neural network is an oriented graph. It consists of nodes which in the biological analogy represent neurons, connected by arcs.\n","\n","Both for classification and regression.\n","\n","**Structure**\n","\n","Neural network may contain the following 3 layers:\n","\n","1. Input layer – The activity of the input units represents the raw information that can feed into the network. They are in the form of various texts, numbers, audio files, image pixels, etc.\n","\n","2. Hidden layer – These hidden layers perform various types of mathematical computation on the input data and the weights on the connections between the input and the hidden units. There may be one or more hidden layers.\n","\n","3. Output layer – The output values are the predictions of the response variable. \n","\n","**Characteristics**\n","\n","Non Linearity: The mechanism followed in ANN for the generation of the input signal is nonlinear.\n","\n","Supervised Learning: The input and output are mapped and the ANN is trained with the training dataset.\n","\n","Unsupervised Learning: The target output is not given, so the ANN will learn on its own by discovering the features in the input patterns.\n","\n","Adaptive Nature: The connection weights in the nodes of ANN are capable to adjust themselves to give the desired output.\n","\n","**Applications:**\n","\n","Handwritten Character Recognition, Speech Recognition, Signature Classification, Facial Recognition,...\n","\n","\n","For supervised learning such as classification, image recognition, the flow of information is from the input layer to the hidden layer and finally to the output. It is called FeedForward Artificial Neural Networks\n","\n","By adding 1 or more hidden layers between the input and output layers and units in this layer the predictive power of neural network increases. But a number of hidden layers should be as small as possible. This ensures that neural network does not store all information from learning set but can generalize it to avoid overfitting.\n"]},{"cell_type":"markdown","metadata":{"id":"k6xep7uE7_16"},"source":["## Single-Layer Feed-Forward Network, Multi-Layer Feed-Forward Network\n"]},{"cell_type":"markdown","metadata":{"id":"81q4o890E93B"},"source":["# **Library**\n","class sklearn.neural_network.MLPClassifier(hidden_layer_sizes=(100, ), activation=’relu’, solver=’adam’, alpha=0.0001, batch_size=’auto’, learning_rate=’constant’, learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10)\n","\n","a) hidden_layer_sizes : tuple, length = n_layers - 2, default (100,). The ith element represents the number of neurons in the ith hidden layer.\n","\n","b) activation : {‘identity’, ‘logistic’, ‘tanh’, ‘relu’}, default ‘relu’\n","\n","c) solver : {‘lbfgs’, ‘sgd’, ‘adam’}, default ‘adam’. The default solver ‘adam’ works pretty well on relatively large datasets \n","(with thousands of training samples or more) in terms of both training time and \n","validation score. For small datasets, however, ‘lbfgs’ can converge faster and perform better.\n","\n","d) alpha, L2 penalty (regularization term) parameter.\n","\n","e) btach_size, When set to “auto”, batch_size=min(200, n_samples)\n","\n","f) learning_rate : {‘constant’, ‘invscaling’, ‘adaptive’}, default ‘constant’, Only used when solver='sgd'.\n","\n","g) verbose : bool, optional, default False, Whether to print progress messages to stdout.\n","\n","h) validation_fraction.\n","\n",".fit(X,y)\n","\n","X : array-like or sparse matrix, shape (n_samples, m_features). The input data.\n","\n","y : array-like, shape (n_samples,) or (n_samples, n_outputs). The target values (class labels in classification, real numbers in regression\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7NrE7Ku_E83Z"},"outputs":[],"source":["import pandas as pd # for data manipulation\n","import numpy as np # for data manipulation\n","#%matplotlib inline\n","import matplotlib.pyplot as plt\n","import seaborn as sns; sns.set()\n","from sklearn.model_selection import train_test_split # for splitting the data into train and test samples\n","from sklearn.metrics import classification_report # for model evaluation metrics\n","from sklearn import metrics\n","from sklearn.preprocessing import OrdinalEncoder # for encoding categorical features from strings to number arrays\n","from sklearn.preprocessing import LabelEncoder#for encoding, converting catogroey variables\n","from sklearn.neural_network import MLPClassifier #Multi-layer Perceptron classifier.\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.preprocessing import StandardScaler\n","scaler = StandardScaler()\n","from sklearn.metrics import classification_report, confusion_matrix #Import scikit-learn metrics module for accuracy calculation\n","import pandas as pd \n","from sklearn.metrics import mean_squared_error"]},{"cell_type":"markdown","metadata":{"id":"IbebU0UC4EYq"},"source":["# **Keras ANN for deep learning**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PH8AbbDxEBtk"},"outputs":[],"source":["\n","from keras.models import Sequential\n","from keras.layers import Dense, LSTM, SimpleRNN, Flatten\n","from keras.layers.embeddings import Embedding\n","from keras.layers.convolutional import Conv1D, MaxPooling1D\n","from numpy import loadtxt\n","from keras.layers import Dense"]},{"cell_type":"markdown","metadata":{"id":"DHGNk1zXMTVX"},"source":["## **Iris data Application using sklearn.neural_network import MLPClassifier**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"96TegCDXHhIO"},"outputs":[],"source":["df = sns.load_dataset('iris')\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mkq_5RDdHmCS"},"outputs":[],"source":["sns.pairplot(data=df, hue = 'species')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EpW6gkRhHvbA"},"outputs":[],"source":["#define the taget y and predictor X\n","target = df['species'] #target/response variable y\n","df1 = df.copy()\n","df1 = df1.drop('species', axis =1)\n","\n","## Defining the attributes/predictors\n","X = df1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ojx3VlA5IA0H"},"outputs":[],"source":["#label encoding\n","#\n","le = LabelEncoder()\n","target = le.fit_transform(target)\n","target"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":215,"status":"ok","timestamp":1647648599675,"user":{"displayName":"lihua yao","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg5rCuL_iDZvHl-VqYRc6KCdT0FRCd54wGgy-Ejpg=s64","userId":"00281987792034256062"},"user_tz":420},"id":"oxzF2FziIRCN","outputId":"2cdb3358-51b7-44cb-a187-d222978f919f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training split input-  (120, 4)\n","Testing split input-  (30, 4)\n"]}],"source":["y=target\n","# Splitting the data - 80:20 ratio\n","X_train, X_test, y_train, y_test = train_test_split(X , y, test_size = 0.2, random_state = 42)\n","print(\"Training split input- \", X_train.shape)\n","print(\"Testing split input- \", X_test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I9-JwwFJ9-ls"},"outputs":[],"source":["\n","def MLP(X,y):\n","  result=list()\n","  X_train, X_test, y_train, y_test = train_test_split(X , y, test_size = 0.2, random_state = 42)\n","  alphas = np.linspace(0.01, 2, num=20)\n","  lvc_df = pd.DataFrame(alphas, columns=['c'])\n","  #lvc_df['model'] = lvc_df['c'].apply(lambda c: MLPClassifier(solver='lbfgs', alpha=c, hidden_layer_sizes=(5, 2)).fit(trX, trY))\n","  lvc_df['model'] = lvc_df['c'].apply(lambda c: MLPClassifier(solver='sgd', alpha=c, hidden_layer_sizes=(100, )).fit(X_train, y_train))\n","\n","  lvc_df['score']=lvc_df['model'].apply(lambda model: model.score(X_test, y_test))\n","  filter=lvc_df['score']==max(lvc_df['score'])\n","  bestmodel=lvc_df[filter]['model'].values[0]\n","\n","  train_score=bestmodel.score(X_train,y_train)\n","  test_score=bestmodel.score(X_test,y_test)\n","  return bestmodel, train_score, test_score, X_test,y_test\n","#predict_proba(X)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MnSsIDUoMx8x"},"outputs":[],"source":["y=target\n","bestmodel, train_score, test_score, X_test,y_test=MLP(X,y)\n","y_pred=bestmodel.predict(X_test)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sTOqfwkBQHRf"},"outputs":[],"source":["cm = confusion_matrix(y_test, y_pred)\n","plt.figure(figsize=(5,5))\n","sns.heatmap(data=cm,linewidths=.5, annot=True,square = True,  cmap = 'Blues')\n","plt.ylabel('Actual label')\n","plt.xlabel('Predicted label')\n","all_sample_title = 'Accuracy Score: {0}'.format(bestmodel.score(X_test, y_test))\n","plt.title(all_sample_title, size = 15)"]},{"cell_type":"markdown","metadata":{"id":"1RtYkBy9M2yG"},"source":["# **More data set at **\n","https://github.com/mwaskom/seaborn-data\n","\n","df=pd.read_csv(\"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/raw/planets.csv\")\n","\n","https://soumenatta.medium.com/analyzing-pima-indians-diabetes-data-using-python-89a021b5f4eb"]},{"cell_type":"markdown","metadata":{"id":"QTOemjGTnx_A"},"source":["## Keras for indian diabeta data\n","Keras is a neural network Application Programming Interface (API) for Python.\n","It is perfect for those that do not have a strong background in Deep Learning, but still want to work with neural networks\n","\n","In any neural network, a dense layer is a layer that is deeply connected with its preceding layer, which means the neurons of the layer are connected to every neuron of its preceding layer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SroRurpYK9jS"},"outputs":[],"source":["# load the dataset\n","#dataset = loadtxt('pima-indians-diabetes.csv', delimiter=',')\n","filein=\"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n","\n","df=pd.read_csv(filein,header=None)\n","df.columns=['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\n","\n","target = df['Outcome'] #target/response variable y\n","df1 = df.copy()\n","df1 = df1.drop('Outcome', axis =1)\n","\n","## Defining the attributes/predictors\n","X = df1\n","y=target"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xNg5f9Fwi3FM"},"outputs":[],"source":["temp=pd.DataFrame(X)\n","temp.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IyOi4s5QMsxM"},"outputs":[],"source":["# Compute correlation matrix \n","correlations = df.corr(method = 'pearson') \n","print(correlations)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XcpJhxdSNCY3"},"outputs":[],"source":["# Import required package \n","from matplotlib import pyplot\n","# set the figure size\n","pyplot.rcParams['figure.figsize'] = [20, 10]\n","# Draw histograms for all attributes \n","df.hist()\n","pyplot.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yl2If4JaN09x"},"outputs":[],"source":["df.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W4PtangtOu4Y"},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","scaler = StandardScaler()\n","scaler.fit(df1)\n","X=scaler.transform(df1)\n","\n","#get_feature_names_out(input_features=df1.columns.values)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VtPiQSaYPD0h"},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(X , y, test_size = 0.2, random_state = 42)\n"," "]},{"cell_type":"markdown","metadata":{"id":"_JiuNgXJrEEu"},"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pxb9TFHmijvr"},"outputs":[],"source":["temp=pd.DataFrame(X_train)\n","temp.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SXxkDlgsNW-O"},"outputs":[],"source":["from numpy import loadtxt\n","from keras.models import Sequential\n","from keras.layers import Dense\n","# split into input (X) and output (y) variables\n","\n","# define the keras model\n","model = Sequential()\n","model.add(Dense(12, input_dim=8, activation='relu'))\n","#model.add(Dense(2, activation='relu'))\n","model.add(Dense(8, activation='relu'))\n","model.add(Dense(8, activation='relu'))\n","model.add(Dense(1, activation='sigmoid'))\n","print(model.summary())\n","# compile the keras model\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) #multi class\n","# fit the keras model on the dataset\n","#model.fit(X, y, epochs=20, batch_size=100)\n","# evaluate the keras model\n","#_, accuracy = model.evaluate(X, y)\n","#print('Accuracy: %.2f' % (accuracy*100))\n","\n","\n","his=model.fit(X_train, y_train, epochs=20, batch_size=32,validation_data=(X_test, y_test))\n","# evaluate the keras model\n","_, accuracy = model.evaluate(X, y)\n","print('Accuracy: %.2f' % (accuracy*100))\n","plt.plot(his.history['loss'])\n","plt.plot(his.history['val_loss'])\n","plt.title('Curve')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'val'], loc='upper left')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0oQ908RGObdM"},"outputs":[],"source":["# make class predictions with the model\n","predictions = (model.predict(X) > 0.5).astype(int)\n","# summarize the first 5 cases\n","for i in range(5):\n","\tprint('%s => %d (expected %d)' % (df1.loc[i].tolist(), predictions[i], y[i]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0m-ATGwFJyjp"},"outputs":[],"source":["cnf_matrix = confusion_matrix(y, predictions)\n","cnf_matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wRWbZF3TConE"},"outputs":[],"source":["class_names=[0,1] # name  of classes\n","\n","fig, ax = plt.subplots(figsize=(10, 5))\n","tick_marks = np.arange(len(class_names))\n","plt.xticks(tick_marks, class_names)\n","plt.yticks(tick_marks, class_names)\n","# create heatmap\n","sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n","ax.xaxis.set_label_position(\"top\")\n","plt.tight_layout()\n","plt.title('Confusion matrix', y=1.1)\n","plt.ylabel('Actual label')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xaS7rM5fdMUt"},"outputs":[],"source":["\n","plt.figure(figsize=(10,5))\n","sns.heatmap(data=cnf_matrix,linewidths=.5, annot=True,square = True,  cmap = 'Blues')\n","plt.ylabel('Actual label')\n","plt.xlabel('Predicted label')\n","plt.tight_layout()\n","_, accuracy=model.evaluate(X_test,y_test)\n","all_sample_title = 'Accuracy Score: {0}'.format(accuracy)\n","plt.title(all_sample_title, size = 15)"]},{"cell_type":"markdown","metadata":{"id":"k5z868mW2wm_"},"source":["##  **MLPClassifier for indian diabeta data**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0hR0Fb-_eIyN"},"outputs":[],"source":["\n","def MLP(X,y):\n","  result=list()\n","  X_train, X_test, y_train, y_test = train_test_split(X , y, test_size = 0.2, random_state = 42)\n","  alphas = np.linspace(0.01, 2, num=20)\n","  lvc_df = pd.DataFrame(alphas, columns=['c'])\n","  #lvc_df['model'] = lvc_df['c'].apply(lambda c: MLPClassifier(solver='lbfgs', alpha=c, hidden_layer_sizes=(5, 2)).fit(trX, trY))\n","  lvc_df['model'] = lvc_df['c'].apply(lambda c: MLPClassifier(solver='sgd', alpha=c, hidden_layer_sizes=(100, )).fit(X_train, y_train))\n","\n","  lvc_df['score']=lvc_df['model'].apply(lambda model: model.score(X_test, y_test))\n","  filter=lvc_df['score']==max(lvc_df['score'])\n","  bestmodel=lvc_df[filter]['model'].values[0]\n","\n","  train_score=bestmodel.score(X_train,y_train)\n","  test_score=bestmodel.score(X_test,y_test)\n","  return bestmodel, train_score, test_score, X_test,y_test\n","#predict_proba(X)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1iVUPLDSenUD"},"outputs":[],"source":["\n","bestmodel, train_score, test_score, X_test,y_test=MLP(X,y)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z3zx5ASUfvey"},"outputs":[],"source":["y_pred=bestmodel.predict(X)\n","cnf_matrix = confusion_matrix(y, y_pred)\n","cnf_matrix\n","mse_krm=mean_squared_error(y, y_pred)\n","print(mse_krm)\n","plt.figure(figsize=(5,5))\n","sns.heatmap(data=cnf_matrix,linewidths=.5, annot=True,square = True,  cmap = 'Blues')\n","plt.ylabel('Actual label')\n","plt.xlabel('Predicted label')\n","all_sample_title = 'Accuracy Score: {0}'.format(bestmodel.score(X, y))\n","plt.title(all_sample_title, size = 15)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vIdsipEbunMz"},"outputs":[],"source":["#specify size of heatmap\n","fig, ax = plt.subplots(figsize=(10, 5))\n","\n","#create seaborn heatmap\n","ax = sns.heatmap(cnf_matrix/np.sum(cnf_matrix), annot=True, \n","            fmt='.2%', cmap='Blues')\n","\n","ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n","ax.set_xlabel('\\nPredicted ')\n","ax.set_ylabel('Actual ');\n","\n","## Display the visualization of the Confusion Matrix.\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ctCIIZm_stvx"},"outputs":[],"source":["#specify size of heatmap\n","fig, ax = plt.subplots(figsize=(10, 5))\n","\n","labels = ['True Neg','False Pos','False Neg','True Pos']\n","labels = np.asarray(labels).reshape(2,2)\n","sns.heatmap(cnf_matrix, annot=labels, fmt='', cmap='Blues')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"azxk_hqvfI9R"},"outputs":[],"source":["y_pred=bestmodel.predict(X_test)\n","cnf_matrix = confusion_matrix(y_test, y_pred)\n","cnf_matrix\n","mse_krm=mean_squared_error(y_test, y_pred)\n","print(mse_krm)\n","\n","plt.figure(figsize=(5,5))\n","sns.heatmap(data=cnf_matrix,linewidths=.5, annot=True,square = True,  cmap = 'Blues')\n","plt.ylabel('Actual label')\n","plt.xlabel('Predicted label')\n","all_sample_title = 'Accuracy Score: {0}'.format(bestmodel.score(X_test, y_test))\n","plt.title(all_sample_title, size = 15)\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J60jRJ24rw8G"},"outputs":[],"source":["from sklearn.metrics import plot_confusion_matrix\n","plot_confusion_matrix(bestmodel, X_test, y_test)\n","plt.show()  "]},{"cell_type":"markdown","metadata":{"id":"Nu6SDD03MC6v"},"source":["#**HW Diamonds data application,encode categoral data**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xFkWcu-vovE2"},"outputs":[],"source":["df = sns.load_dataset('diamonds')\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PZUt1TXfo255"},"outputs":[],"source":["#define the taget y and predictor X\n","target = df['price'] #target/response variable y\n","df1 = df.copy()\n","df1 = df1.drop('price', axis =1)\n","\n","## Defining the attributes/predictors\n","X = df1\n","X = X.astype(str)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tby1LY0pqHNN"},"outputs":[],"source":["#The best practice when encoding variables is to fit the encoding on the training dataset,\n","# then apply it to the train and test datasets.\n","# prepare input data\n","def prepare_inputs(X_train, X_test,enccolname,colname):\n","  oe = OrdinalEncoder()\n","  oe.fit(X_train[enccolname])\n","  X_train_enc = oe.transform(X_train[enccolname])\n","  X_test_enc = oe.transform(X_test[enccolname])\n","  df2=pd.DataFrame(X_train_enc)\n","  df1=pd.DataFrame(X_train[colname])\n","  X_train_enc=df1.reset_index(drop=True).merge(df2.reset_index(drop=True), left_index=True, right_index=True)\n","  df2=pd.DataFrame(X_test_enc)\n","  df1=pd.DataFrame(X_test[colname])\n","  X_test_enc=df1.reset_index(drop=True).merge(df2.reset_index(drop=True), left_index=True, right_index=True)\n","  X_train_enc=scaler.fit_transform(X_train_enc)\n","  X_test_enc=scaler.transform(X_test_enc)\n","  return X_train_enc, X_test_enc\n","\n","\n","# prepare target\n","def prepare_targets(y_train, y_test):\n","\tle = LabelEncoder()\n","\tle.fit(y_train)\n","\ty_train_enc = le.transform(y_train)\n","\ty_test_enc = le.transform(y_test)\n","\treturn y_train_enc, y_test_enc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OqyrVeJfsUqk"},"outputs":[],"source":["def MLP(X,y):\n","  result=list()\n","  X_train, X_test, y_train, y_test = train_test_split(X , y, test_size = 0.2, random_state = 42)\n","  X_train_enc, X_test_enc = prepare_inputs(X_train, X_test,enccolname=['cut','color','clarity'],colname=['carat','depth','table','x','y','z'])\n","  alphas = np.linspace(1, 2, num=2)\n","  lvc_df = pd.DataFrame(alphas, columns=['c'])\n","  #lvc_df['model'] = lvc_df['c'].apply(lambda c: MLPClassifier(solver='lbfgs', alpha=c, hidden_layer_sizes=(5, 2)).fit(trX, trY))\n","  lvc_df['model'] = lvc_df['c'].apply(lambda c: MLPClassifier(solver='sgd', alpha=c, hidden_layer_sizes=(100, )).fit(X_train_enc, y_train))\n","\n","  lvc_df['score']=lvc_df['model'].apply(lambda model: model.score(X_test_enc, y_test))\n","  filter=lvc_df['score']==max(lvc_df['score'])\n","  bestmodel=lvc_df[filter]['model'].values[0]\n","\n","  train_score=bestmodel.score(X_train_enc,y_train)\n","  test_score=bestmodel.score(X_test_enc,y_test)\n","  return bestmodel, train_score, test_score, X_train_enc, X_test_enc,y_train, y_test\n","#predict_proba(X)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"TfuCtClHpGFU","outputId":"83d92729-39d2-4260-ea17-239c0943350e"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:1692: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:1692: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:1692: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]}],"source":["y=target\n","bestmodel, train_score, test_score, X_train_enc, X_test_enc,y_train, y_test=MLP(X,y)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"je8qDvBG9cbJ","outputId":"9137790a-df41-4839-8e21-60d7da411dd5"},"outputs":[{"name":"stdout","output_type":"stream","text":["10804429.922784576\n"]}],"source":["y_pred=bestmodel.predict(X_test_enc)\n","cnf_matrix = confusion_matrix(y_test, y_pred)\n","cnf_matrix\n","mse_krm=mean_squared_error(y_test, y_pred)\n","print(mse_krm)\n","\n","plt.figure(figsize=(5,5))\n","sns.heatmap(data=cnf_matrix,linewidths=.5, annot=True,square = True,  cmap = 'Blues')\n","plt.ylabel('Actual label')\n","plt.xlabel('Predicted label')\n","all_sample_title = 'Accuracy Score: {0}'.format(bestmodel.score(X_test_enc, y_test))\n","plt.title(all_sample_title, size = 15)\n","plt.legend()\n","plt.show()"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Class9_ANN.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}